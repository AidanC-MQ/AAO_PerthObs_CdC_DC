{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"PIfXUqQYTUHA"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"i0VPSyLig185","executionInfo":{"status":"ok","timestamp":1760065867401,"user_tz":-660,"elapsed":1620,"user":{"displayName":"Aidan Connaughton","userId":"18276585510061555477"}},"outputId":"8feb1916-d90a-40fa-e234-e02bf7cef2ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mBadData\u001b[0m/                                                     plate177.fits\n","New_Master_Metadata.csv                                      plate182.fits\n","plate103.fits                                                plate189.fits\n","plate103_nocoords_fast_grid_removed_fast_grid_removed.fitss  plate192.fits\n","plate103_nocoords.fits                                       plate196.fits\n","plate106.fits                                                plate199.fits\n","plate121.fits                                                plate200.fits\n","plate142.fits                                                plate205.fits\n","plate147.fits                                                plate210.fits\n","plate158.fits                                                plate218.fits\n","plate161.fits                                                plate230.fits\n","plate166.fits                                                plate237.fits\n","plate170.fits                                                plate251.fits\n","plate173.fits                                                \u001b[01;34mupdated_fits\u001b[0m/\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/AAO_PACE\n"]}],"source":["\n","\n","import os, sys, math, glob, csv\n","#!pip install pyplate\n","\n","from astropy.io import fits\n","from astropy.wcs import WCS\n","import astropy.units as u\n","from astropy.coordinates import SkyCoord, FK4, FK5, Angle\n","from astropy.time import Time\n","import astropy.coordinates as acoord\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import itertools\n","\n","from scipy import ndimage\n","import shutil\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=False)\n","%cd /content/gdrive/MyDrive/AAO_PACE/\n"," # 'MyDrive' is root of your google drive\n"]},{"cell_type":"code","source":[],"metadata":{"id":"J6BJ-TygGVoX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## IF NOT RUNNING LOCALLY\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"2cAj9iX7ggtJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761711405269,"user_tz":-660,"elapsed":20771,"user":{"displayName":"Matthew Lee","userId":"04784831902257950887"}},"outputId":"581e6095-4542-48a9-9c85-b1010875ce8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# = = = = =\n","# MASTER CODE FOR FINAL PRODUCT\n","# = = = = =\n","\n","def Update_Perth_Plate_Fits(input_folder_path = '/content/gdrive/MyDrive/AAO_PACE/', logfile = True):\n","\n","\n","  def WCSFitsUpdate(loadfits = input_folder_path):\n","\n","    def var_cleanup():\n","\n","      local_vars = ('head','fitsimport','ObsTime','ObsJD','ExpTime','HeadEquinox','missingdata', 'obsRA', 'obsDEC', 'raHead', 'decHead')\n","      for i in local_vars:\n","        if i in locals():\n","          del locals()[i]\n","      # clear out some vars to avoid errors in next code run\n","      return\n","\n","    def NoGo_badData(fitsfile):\n","\n","\n","        badcsv = 'FailedFits.csv'\n","\n","        print(f'{fitsfile} missing data, name appended to {badcsv}')\n","        if not os.path.exists(badcsv):    #create csv if doesnt exist\n","          with open(badcsv, 'w', newline='') as csvfile:\n","            writer = csv.writer(csvfile)\n","            writer.writerow(['Files with Missing/broken Data'])\n","\n","        with open(badcsv, 'a', newline='') as csvfile:\n","          writer = csv.writer(csvfile)  # append line\n","          writer.writerow([fitsfile])\n","\n","        return\n","\n","    def addToLog(string):\n","      # Generate Logfile\n","      if logfile == True:\n","        log = f'logfile.txt'\n","        # if not os.path.exists(log):    #create csv if doesnt exist -=- open already creates file if none exists\n","        with open(log, 'a') as logg:\n","          logg.write(f'{string}\\n')\n","          logg.flush()  # it'd be impolite not to\n","      return\n","\n","    if not os.path.exists('logfile.txt'):\n","      addToLog('##  Australian Astronomical Optics  Perth Observatory  ##')\n","      addToLog('##        Photographic Plates Ingestion Script         ##')\n","      addToLog('  MQ PACE Placement 2025')\n","      addToLog('  nuwanthika.fernando@mq.edu.au,  aidan.connaughton@mq.edu.au , matthew.lee20@students.mq.edu.au ')\n","\n","\n","    # # # # # # # # #\n","    # Actually running the code now!\n","    # # # # # # # # #\n","\n","    WCS_EXISTS = 0\n","    missingdata = 0\n","\n","    head = []\n","\n","    pathdir, fitsfile = os.path.split(loadfits) # split into   dirdirdir /, file\n","\n","    base, ext = os.path.splitext(fitsfile)       # split into filename , .extension\n","\n","    addToLog('\\n# # # # # #\\nSTARTING  {fitsfile}\\n# # # # #')\n","    fitsimport = fits.open(loadfits, ignore_missing_simple=True)\n","\n","\n","\n","    if fitsimport[0].header[0] != True and fitsimport[0].header[0] != 'T':\n","      print('No SIMPLE header in PrimaryHDU (main image)')\n","      addToLog('No SIMPLE Keyword found in 1st Header!')\n","      head = fitsimport[1].header\n","      if fitsimport[1].header[0] != True and fitsimport[1].header[0] != 'T':\n","        print('No SIMPLE header in ImageHDU (envelope)')\n","        missingdata = 1\n","        NoGo_badData(fitsfile)\n","        addToLog('No SIMPLE Keyword found in either 2nd header!')\n","      return\n","    else:\n","      head = fitsimport[0].header\n","\n","    if \"RA\" not in head or \"DEC\" not in head:\n","      addToLog('RA or DEC missing from header!')\n","      missingdata = 1\n","    else:\n","      ObsRA       = head[\"RA\"]\n","      ObsDEC      = head[\"DEC\"]\n","      if (ObsRA < 0) or (ObsRA > 24):\n","       addToLog('RA value is bad! (below 0, o r above 24 hr)')\n","       missingdata = 1\n","\n","      if (ObsDEC < -90) or (ObsDEC > 90):\n","       addToLog('DEC value is bad! (below -90, or above +90 deg)')\n","       missingdata = 1\n","\n","\n","    if \"JD\" in head:\n","      ObsJD = head[\"JD\"]\n","    else:\n","        if \"DATE-OBS\" in head:\n","          # ObsJD = head[\"DATE-OBS\"].time.format = 'jd'   #if no JD, set the date to JD and use that instead... -- Threw an error so I added dateutils.parse\n","          dt = parse(head['DATE-OBS']);\n","          ObsJD = Time(dt).jd;\n","          addToLog('DATE-OBS used in place of JD')\n","        else:\n","          missingdata = 1\n","          addToLog('No JD or DATE-OBS found!')\n","\n","    if (ObsJD < 2411368.5  )  or  (ObsJD > 2422324.5):  #\n","        missingdata = 1\n","        addToLog('JD outside valid range ( 1/1/1890 < JD < 1/1/1920) ')\n","\n","\n","    if 'CRPIX'  in head:\n","      addToLog('CRPIX found in header, WCS already exists, not generating WCS!')\n","      ###\n","      WCS_EXISTS = 1\n","      ###\n","    else:\n","      WCS_EXISTS = 0\n","\n","\n","    if missingdata == 1:  # last check before copying and starting\n","      addToLog('###\\n File Aborted Early, MissingData flag triggered!\\n###')\n","      NoGo_badData(fitsfile)\n","      var_cleanup()\n","      return 'BAD_DATA'\n","\n","\n","    if missingdata == 0:\n","      fitsimport[0].header = head\n","      fitsimport.close()\n","\n","      # Create the new file now!\n","      os.makedirs('updated_fits', exist_ok=True)\n","      new_name = f\"updated_fits/{base}_copy{ext}\"            # append \"_copy\"\n","      shutil.copy(loadfits, new_name)              # create copy with appended text\n","      workableFits = fits.open(new_name, ignore_missing_simple=True)\n","      if workableFits[0].header[0] == True or workableFits[0].header[0] == 'T':\n","          head = workableFits[0].header\n","      elif workableFits[0].header[1] == True or workableFits[0].header[1] == 'T':\n","          head = workableFits[1].header\n","      else:\n","          print('Error creating new fits for',new_name)\n","          return 'BAD_DATA'\n","\n","\n","      # Convert RA/DEC to ICRS\n","      # J2000 -> ICRS   Follows FK4NoETerm under SOFA\n","      ObsRA = ObsRA *u.hourangle\n","      ObsDEC = ObsDEC*u.deg\n","      ObstimeCalc = Time(ObsJD, format = 'jd', scale = 'tt')\n","      cJD = SkyCoord(ra=ObsRA, dec=ObsDEC, frame=FK4(equinox='B1950.0', obstime = ObstimeCalc))\n","      c_ICRS = cJD.transform_to('icrs')\n","      # Set new values\n","      raHead = Angle(c_ICRS.ra, unit = u.hourangle).deg\n","      decHead = Angle(c_ICRS.dec, unit = u.deg).deg  # take to 2 sig fig!!!!!!\n","      ################\n","      addToLog('RA/DEC Looks good! \\n')\n","      addToLog(f'Original values (dec hr, dec deg): {ObsRA}, {ObsDEC}')\n","      addToLog(f'O. values (sexg. hr, sexg. deg)  : {cJD.ra.to_string(unit = u.hourangle, sep=\":\" )}, {cJD.dec.to_string(unit = u.deg, sep=\":\" )} \\n')\n","      addToLog(f'ICRS: {c_ICRS.ra.to_string(unit=u.hourangle, sep=\":\", precision=2)}, {c_ICRS.dec.to_string(unit=u.deg, sep=\":\", precision=2)} ')\n","\n","      #Check existing RA/DEC/WCS wrt image metadate (eg. already solved image)\n","      # Check if +/- 2.5\n","      #if head[\"RA\"]:\n","\n","\n","    # # # # # # #\n","    # WCS Time! #\n","    # # # # # # #\n","\n","    # if no wcs exists (allow existing data to be entered)                                #### #### #### #### ####\n","\n","\n","\n","\n","    def AddConvWCS(head, CRPIX = [ head['NAXIS1'] /2 , head['NAXIS2'] /2 ] ,RADESYS = \"ICRS\",CTYPE = [\"RA---TAN\", \"DEC--TAN\"],CRVAL = [raHead, decHead],RA = raHead,DEC = decHead):\n","          # RA/DEC_NEW AS WELL, FOR THE CRVAL, LEAVE ORIG RA/DEC KEYWORDS\n","          # head var must be active and open\n","          # WCS fallback for unsolved image/ no wcs already present\n","\n","          # TODO:\n","          #       Allow image with existing 'solved' WCS data to be accept and not overwritten!\n","        w = WCS(naxis=2)\n","\n","        if missingdata == 0:\n","          w = WCS(naxis=2)\n","\n","          # set image centre\n","          w.wcs.crpix = CRPIX       #[ head['NAXIS1'] /2 , head['NAXIS2'] /2 ]\n","          w.wcs.radesys = RADESYS   #\"ICRS\"\n","          w.wcs.ctype = CTYPE       #[\"RA---TAN\", \"DEC--TAN\"]    # add comment\n","\n","          w.wcs.crval = [RA,DEC]    #[raHead, decHead]\n","\n","          '''\n","          CD matrix - CROT deprecated, rotation now integrated to distortion matrix\n","          [-pxscale, 0]       RA typically increases to left of frame ->  -ve pxscale\n","          [0 , pxscale]\n","          create comment to address potential image flip\n","          CDELT is translation, PC is rotation\n","          '''\n","          #if not head['CD']:   'doesnt exist in header - yeah thats the point...\n","          pxScale =  0.000175#162   deg/px  0.63 \"/px          plate106 solved to (CD1_1\t0.000174758115388) (CD2_2\t0.00017556584632)\n","          w.wcs.cd = np.array([[pxScale, 0 ], # array elements are unknown-signed, plate orientation cannot be assumed for all\n","                                  [0, pxScale ]])  # fits comment???\n","            #w.wcs.cd = CD\n","\n","          wcs_header = w.to_header()\n","          head.update(wcs_header)\n","\n","          del head['MJDREF']    # mjdref = 0, no offset being applied, just remove it to save confusion\n","\n","          head.set('hierarch AAO_COMMENT_PX1', 'PC[N_N] VALUES UNSIGNED, NO CONSISTENT SCAN ORIENTATION')\n","          head.set('hierarch AAO_COMMENT_PX2', 'PXSCALE ~0.631 \"/PX FROM ASTROMETRY.NET SOLVES')\n","\n","        return head\n","\n","\n","    if WCS_EXISTS == 0:\n","      head = AddConvWCS(head)\n","      if \"SIMPLE\" in head:\n","        head[\"SIMPLE\"] = True\n","        addToLog('SIMPLE keyword found, set to True')\n","    ########################################################################################  SET UP INPUT FOR EXISTING WCS\n","\n","    # # #\n","    # WCS Time is now over.\n","    # # #\n","    timenow = Time.now().to_string()\n","    head.set('hierarch AAO_COMMENT1', 'b19xx PRECESSED VIA ASTROPY TO ICRS')\n","    head.set('hierarch AAO_COMMENT2', 'AAO DATACENTRAL PRE-INGESTION, PROCESSED:')\n","    head.set('hierarch AAO_COMMENT3', f'{timenow} UTC')\n","\n","    workableFits[0].header = head\n","    workableFits.writeto(f'{new_name}' ,overwrite = True)\n","\n","    addToLog(f'{base}_copy generated successfully!\\n @ {timenow} UTC' )\n","    addToLog(f'Output Header: \\n{repr(head)}')\n","\n","    workableFits.close()\n","    var_cleanup()\n","\n","\n","\n","    return new_name # output new fits filepath to be added to metadata csv\n","\n","\n","\n","  ###-=-=-=-=-=-=-=-=-=-\n","  ### METADATA SECTION\n","  ###-=-=-=-=-=-=-=-=-=-\n","\n","\n","\n","  def error_check(var):\n","    ''' Check if a variable has been set to ERROR as an escape so code doesn't crash,\n","    prints the error and returns True if an error occured '''\n","\n","    if var[:14] == 'METADATA ERROR':\n","      print(var)\n","      return True\n","    else:\n","      return False\n","\n","  def check_dupes(lst):\n","    seen = []\n","    dupes = []\n","    dupe_counts = []\n","    dupes_total = []\n","    dupe_indexs = []\n","    for j,x in enumerate(lst):\n","      if x in seen:\n","        dupes_total.append(x)\n","        dupe_indexs.append(j)\n","\n","        if x not in dupes:\n","          dupes.append(x)\n","          dupe_counts.append(1)\n","        for i,dupe in enumerate(dupes):\n","          if x == dupe:\n","            dupe_counts[i] += 1\n","      else:\n","        seen.append(x)\n","    return dupes, dupe_counts, dupes_total, dupe_indexs\n","\n","\n","\n","  def unpack_fits(fits):\n","    ''' Extracts the headers and data from an input fits file '''\n","\n","    # Make sure a fits file is input so the code doesn't crash        #   .fit files are a thing that exist, genernally not the \"\"standard\"\" but is common enough\n","    if fits[-5:] != '.fits':\n","      return 'METADATA ERROR - Please input a valid .fits file'\n","\n","    # Use pyplate to extract the values from the fits\n","    return pp.metadata.PlateHeader.from_fits(fits)\n","\n","\n","\n","  def unpack_csv_headers(file):\n","    ''' Extracts the headers from an input csv file '''\n","\n","    # Make sure a csv file is input so the code doesn't crash\n","    if file[-4:] != '.csv':\n","      return 'METADATA ERROR - Please input a valid .csv file'\n","\n","    # Extract the first row of the csv to get the headers\n","    with open(file, newline='') as csvfile:\n","      return list(csv.reader(csvfile))[0]\n","\n","\n","\n","  def create_master_from_fits(fits):\n","    # Use an example fits to start a master_file\n","    fits_total = unpack_fits(fits)\n","    heads = list(fits_total.keys())\n","    heads.insert(0,'file_name')\n","    masterdf = pd.DataFrame(columns=heads)\n","    masterdf.to_csv('New_Master_Metadata.csv',index=False)\n","    return\n","\n","\n","\n","  def add_header(header_name, header_list):\n","    ''' Adds a header of input name at the end of an input header list '''\n","\n","    header_list.append(header_name)\n","\n","\n","\n","  def add_fits_to_master(fits,master_file):\n","\n","    ## ADD CURRENT MASTER HEADERS\n","\n","    master_headers = unpack_csv_headers(master_file)\n","    if error_check(master_headers):\n","      return\n","\n","\n","    ## ADD FITS\n","\n","    fits_total = unpack_fits(fits)\n","    if error_check(fits_total):\n","      return\n","\n","\n","    ## ADD FITS HEADERS\n","\n","    cur_heads = list(fits_total.keys())\n","\n","\n","    ## CHECK IF HEADERS NEED TO BE ADDED TO MASTER\n","\n","    # Make sure duplicates are the same\n","    cur_dupes, cur_dupe_counts, cur_dupe_totals, cur_dupe_indexs = check_dupes(cur_heads)\n","    master_dupes, master_dupe_counts , master_dupe_totals, master_dupe_indexs = check_dupes(master_headers)\n","\n","    for i,dupe in enumerate(cur_dupes):\n","      if dupe not in master_dupes:\n","        add_header(dupe,master_headers)\n","      else:\n","        if cur_dupe_counts[i] > master_dupe_counts[i]:\n","          for _ in range(cur_dupe_counts[i] - master_dupe_counts[i]):\n","            add_header(dupe,master_headers)\n","\n","    # Add any new headers to master\n","    added_headers=[]\n","    for i,name in enumerate(cur_heads):\n","      if name not in master_headers:\n","        master_headers.append(name)\n","        added_headers.append([i,name])\n","\n","    # Set new total dupes so data can be added later\n","    new_master_dupes, new_master_dupe_counts, new_master_dupe_totals, new_master_dupe_indexs = check_dupes(master_headers)\n","\n","\n","    ## ADD FITS DATA\n","\n","    cur_data = list(fits_total.values())\n","\n","\n","    ## ADD DATA TO MASTER\n","\n","    # Create a list the same length as the master so it can add correctly and any missing data is nil\n","    input_data = ['nil'] * len(master_headers)\n","\n","    # Add the name of the plate to the first index of the metadata\n","    input_data[0] = os.path.splitext(os.path.split(fits)[1])[0]\n","\n","    # Go through each header in master to add the corresponding value - without duplicates\n","    for i,name in enumerate(master_headers):\n","      if name in cur_heads and name not in cur_dupes:\n","        input_data[i+1] = fits_total[name]\n","\n","    # Go back through and add duplicate datas to corresponding\n","    for i,name in enumerate(cur_dupe_totals):\n","      for j,typ in enumerate(new_master_dupe_totals):\n","        if name == typ and input_data[new_master_dupe_indexs[j]] == 'nil':\n","          input_data[new_master_dupe_indexs[j]] = cur_data[cur_dupe_indexs[i]]\n","\n","    # Append the input data to the master data\n","    with open(master_file, newline='') as csvfile:\n","      out_csv = list(csv.reader(csvfile))\n","\n","    out_csv.append(input_data)\n","\n","    out_df = pd.DataFrame(out_csv[1:],columns=master_headers)\n","    out_df.to_csv('New_Master_Metadata.csv',index=False)\n","    print('Sucessfully added',fits,'to metadata')\n","\n","  def find_and_wcs_fits():\n","    fitslist = glob.glob('**/*.fit*', recursive = True,)\n","\n","    created_master = False\n","    for i,found_path in enumerate(fitslist):\n","      pathdir, fitsfile = os.path.split(found_path)\n","      if fitsfile[:5] == 'plate' and pathdir != 'updated_fits' and pathdir != 'BadData':\n","        print('')\n","        print('','Found:',found_path)\n","        updated_file = WCSFitsUpdate(found_path)\n","        print('')\n","        if updated_file != 'BAD_DATA':\n","          if not created_master:\n","            create_master_from_fits(updated_file)\n","            created_master = True\n","          add_fits_to_master(updated_file,'New_Master_Metadata.csv')\n","\n","    return\n","\n","\n","  ###\n","  # End of function setup\n","  ###\n","\n","\n","  print('Checking and locally installing required packages...')\n","  print('')\n","\n","  import os, sys, math, glob, csv;\n","  !pip install pyplate -q;\n","  import pyplate as pp;\n","\n","\n","  from astropy.io import fits;\n","  from astropy.wcs import WCS;\n","  import astropy.units as u;\n","  from astropy.coordinates import SkyCoord, FK4, FK5, Angle;\n","  from astropy.time import Time;\n","  import astropy.coordinates as acoord;\n","  from dateutil.parser import parse\n","\n","  import numpy as np;\n","  import matplotlib.pyplot as plt;\n","  import pandas as pd;\n","  import itertools;\n","\n","  from scipy import ndimage;\n","  import shutil;\n","\n","  print('All packages correct ✓')\n","  print('Locating plates folder...')\n","  print('')\n","\n","  os.chdir(input_folder_path)\n","\n","  print('Working in:', os.getcwd())\n","  print('')\n","\n","  print('Obtaining all plate fits')\n","\n","  find_and_wcs_fits()\n","\n","  print('\\nAll fits conversions done :)')\n","\n","\n","  #! pip install alive-progress\n"],"metadata":{"id":"uMncA-_FiFOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Testing master code\n","Update_Perth_Plate_Fits('/content/gdrive/MyDrive/AAO_PACE/original_fits')"],"metadata":{"id":"gHtMdrprlExr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761715454169,"user_tz":-660,"elapsed":323404,"user":{"displayName":"Matthew Lee","userId":"04784831902257950887"}},"outputId":"51c08e98-077c-40e7-d177-0fe69d6dbd50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking and locally installing required packages...\n","\n","All packages correct ✓\n","Locating plates folder...\n","\n","Working in: /content/gdrive/.shortcut-targets-by-id/1mBGDQpFuenHzfjpH-i8R8SP0vQoDT91T/AAO_PACE/original_fits\n","\n","Obtaining all plate fits\n","\n"," Found: plate142.fits\n","\n","Sucessfully added updated_fits/plate142_copy.fits to metadata\n","\n"," Found: plate147.fits\n","\n","Sucessfully added updated_fits/plate147_copy.fits to metadata\n","\n"," Found: plate121.fits\n","\n","Sucessfully added updated_fits/plate121_copy.fits to metadata\n","\n"," Found: plate103.fits\n","\n","Sucessfully added updated_fits/plate103_copy.fits to metadata\n","\n"," Found: plate158.fits\n","\n","Sucessfully added updated_fits/plate158_copy.fits to metadata\n","\n"," Found: plate106.fits\n","\n","Sucessfully added updated_fits/plate106_copy.fits to metadata\n","\n"," Found: plate196.fits\n","\n","Sucessfully added updated_fits/plate196_copy.fits to metadata\n","\n"," Found: plate200.fits\n","\n","Sucessfully added updated_fits/plate200_copy.fits to metadata\n","\n"," Found: plate182.fits\n","\n","Sucessfully added updated_fits/plate182_copy.fits to metadata\n","\n"," Found: plate237.fits\n","\n","Sucessfully added updated_fits/plate237_copy.fits to metadata\n","\n"," Found: plate173.fits\n","\n","Sucessfully added updated_fits/plate173_copy.fits to metadata\n","\n"," Found: plate205.fits\n","\n","Sucessfully added updated_fits/plate205_copy.fits to metadata\n","\n"," Found: plate192.fits\n","\n","Sucessfully added updated_fits/plate192_copy.fits to metadata\n","\n"," Found: plate166.fits\n","\n","Sucessfully added updated_fits/plate166_copy.fits to metadata\n","\n"," Found: plate170.fits\n","\n","Sucessfully added updated_fits/plate170_copy.fits to metadata\n","\n"," Found: plate189.fits\n","\n","Sucessfully added updated_fits/plate189_copy.fits to metadata\n","\n"," Found: plate251.fits\n","\n","Sucessfully added updated_fits/plate251_copy.fits to metadata\n","\n"," Found: plate218.fits\n","\n","Sucessfully added updated_fits/plate218_copy.fits to metadata\n","\n"," Found: plate210.fits\n","\n","Sucessfully added updated_fits/plate210_copy.fits to metadata\n","\n"," Found: plate199.fits\n","\n","Sucessfully added updated_fits/plate199_copy.fits to metadata\n","\n"," Found: plate230.fits\n","\n","Sucessfully added updated_fits/plate230_copy.fits to metadata\n","\n"," Found: plate161.fits\n","\n","Sucessfully added updated_fits/plate161_copy.fits to metadata\n","\n"," Found: plate177.fits\n","\n","Sucessfully added updated_fits/plate177_copy.fits to metadata\n","\n"," Found: plate103_nocoords.fits\n","plate103_nocoords.fits missing data, name appended to FailedFits.csv\n","\n","\n"," Found: plate103_nocoords_fast_grid_removed_fast_grid_removed.fitss\n","plate103_nocoords_fast_grid_removed_fast_grid_removed.fitss missing data, name appended to FailedFits.csv\n","\n","\n","All fits conversions done :)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"w8peyrv9aPd_"}},{"cell_type":"code","source":["import astropy\n","print(astropy._citation_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"jNsPBsnVaQE7","executionInfo":{"status":"error","timestamp":1760071103191,"user_tz":-660,"elapsed":31,"user":{"displayName":"Aidan Connaughton","userId":"18276585510061555477"}},"outputId":"e33854b0-889e-4628-d0a6-e9a9973f4c58"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'astropy' has no attribute '_citation_'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1972108473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mastropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mastropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_citation_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/astropy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astropy.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'astropy' has no attribute {attr!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'astropy' has no attribute '_citation_'"]}]}]}